program: run.py
method: bayes
name: lm_decoding
entity: machine-learning-hpi
project: brain2text
run_cap: 100
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - ${args}
  - --use_wandb=true
  - --only_test=true
  - --encoder_fc_hidden_sizes=[256]
  - --encoder_gru_hidden_size=512
  - --encoder_num_gru_layers=3
  - --experiment_type=b2p2t_gru+w2v_conformer
  - --loss_function=ctc
  - --early_stopping_patience=10
  - --epochs=100
  - --batch_size=32
  - --learning_rate=0.0001
  - --return_best_model=true
  - --encoder_learnable_inital_state=false
  - --unfreeze_strategy=brain_encoder+w2v
  - --weight_decay=8.324385138271928e-05
  - --encoder_dropout=0.4570249990196249
  - --gaussian_smooth_width=1.5290517142639226
  - --w2v_learning_rate=9.506050391898906e-06
  - --w2v_warmup_steps=7
  - --w2v_warmup_start_step=7
  - --whiteNoiseSD=0.01978441712172472
  - --constantOffsetSD=0.2443028255597108
  - --lm_decode_test_predictions=true
  - --wav2vec_checkpoint=facebook/wav2vec2-conformer-rope-large-960h-ft
  - --tokenizer_checkpoint=facebook/wav2vec2-conformer-rope-large-960h-ft
  - --experiment_name="gru+w2v conformer large"
  - --from_checkpoint=/hpi/fs00/scratch/tobias.fiedler/brain2text/experiment_results/b2p2t_gru+w2v_conformer/2024-08-13_19#13#07/model.pt
  - --visualize_predictions_n_batches=-1
metric:
  name: test_word_error_rate_lm_decode
  goal: minimize
parameters:
  lm_decode_beam_width:
    # Default: 100
    distribution: int_uniform
    min: 20
    max: 200
  lm_decode_beam_prune_logp:
    # Default: -5
    min: -30
    max: 10
  lm_decode_token_min_logp:
    # Default: -5
    min: -20
    max: 20
  lm_decode_alpha:
    min: 0.0
    max: 1.0
  lm_decode_beta:
    min: 0.0
    max: 1.0
  lm_score_boundary:
    values: ["true","false"]