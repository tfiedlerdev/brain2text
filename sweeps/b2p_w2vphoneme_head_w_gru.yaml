program: run.py
method: bayes
name: b2p_encoder_w2v_phoneme
entity: machine-learning-hpi
project: brain2text
run_cap: 100
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - ${args}
  - --experiment_type=b2p_w2vphoneme_head
  - --use_wandb=true
  - --loss_function=ctc
  - --early_stopping_patience=7
  - --epochs=100
  - --batch_size=8
  - --return_best_model=true
  - --unfreeze_strategy=brain_encoder+w2v 
  - --head_fc_hidden_sizes=[512]
  - --head_gru_hidden_size=256
  - --head_gru_num_layers=2
  - --loss_function=ctc
  - --epochs=100
  - --batch_size=8
  - --return_best_model=true
  - --results_subdir_name=no_gru
  - --prepend_gru_to_head=true
  - --optimizer_epsilon=0.1
  - --adjust_global_lr_to_w2v_postwarmup_lr=true
  - --head_checkpoint=/hpi/fs00/scratch/tobias.fiedler/brain2text/experiment_results/a2p_w2vphoneme_head/no_gru/2024-08-16_18#54#26/head.pt
metric:
  name: val_phoneme_error_rate
  goal: minimize
parameters:
  encoder_fc_hidden_sizes:
    values:
      - '[]'
      - '[128]'
      - '[256]'
      - '[512]'
      - '[1024]'
      - '[256, 256]'
      - '[128, 128]'
      - '[512, 125]'
      - '[1024]'
  encoder_gru_hidden_size:
    values:
      - 256
      - 512
      - 756
      - 1024
      - 2048
  encoder_num_gru_layers:
    values:
      - 1
      - 2
      - 3
  encoder_learnable_inital_state:
    values: ["true", "false"]
  encoder_dropout:
    min: 0.0
    max: 0.5
  constantOffsetSD:
    min: 0.0
    max: 0.3
  whiteNoiseSD:
    min: 0.0
    max: 0.1
  weight_decay: 
    min: 0.0
    max: 0.0001
  gaussian_smooth_width:
    min: 0.0
    max: 2.0
  learning_rate:
    min: 0.00000001
    max: 0.02
  w2v_learning_rate:
    min: 0.0000001
    max: 0.0001
  w2v_warmup_steps:
    min: 5
    max: 15
  

# command: python run.py --head_fc_hidden_sizes=[512] --head_gru_hidden_size=256 --head_gru_num_layers=2 --learning_rate=0.00028556295503628926 --experiment_type=b2p_w2vphoneme_head --loss_function=ctc --epochs=100 --batch_size=8 --return_best_model=true --results_subdir_name=no_gru --early_stopping_patience=3 --prepend_gru_to_head=true --optimizer_epsilon=0.1 --head_checkpoint=/hpi/fs00/scratch/tobias.fiedler/brain2text/experiment_results/a2p_w2vphoneme_head/no_gru/2024-08-16_18#54#26/head.pt